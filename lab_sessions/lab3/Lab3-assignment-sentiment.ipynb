{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab3 - Assignment Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright: Vrije Universiteit Amsterdam, Faculty of Humanities, CLTL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes the LAB-2 assignment of the Text Mining course. It is about sentiment analysis.\n",
    "\n",
    "The aims of the assignment are:\n",
    "* Learn how to run a rule-based sentiment analysis module (VADER)\n",
    "* Learn how to run a machine learning sentiment analysis module (Scikit-Learn/ Naive Bayes)\n",
    "* Learn how to run scikit-learn metrics for the quantitative evaluation\n",
    "* Learn how to perform and interpret a quantitative evaluation of the outcomes of the tools (in terms of Precision, Recall, and F<sub>1</sub>)\n",
    "* Learn how to evaluate the results qualitatively (by examining the data) \n",
    "* Get insight into differences between the two applied methods\n",
    "* Get insight into the effects of using linguistic preprocessing\n",
    "* Be able to describe differences between the two methods in terms of their results\n",
    "* Get insight into issues when applying these methods across different  domains\n",
    "\n",
    "In this assignment, you are going to create your own gold standard set from 50 tweets. You will the VADER and scikit-learn classifiers to these tweets and evaluate the results by using evaluation metrics and inspecting the data.\n",
    "\n",
    "We recommend you go through the notebooks in the following order:\n",
    "* **Read the assignment (see below)**\n",
    "* **Lab3.2-Sentiment-analysis-with-VADER.ipynb**\n",
    "* **Lab3.3-Sentiment-analysis.with-scikit-learn.ipynb**\n",
    "* **Answer the questions of the assignment (see below) using the provided notebooks and submit**\n",
    "\n",
    "In this assignment you are asked to perform both quantitative evaluations and error analyses:\n",
    "* a quantitative evaluation concerns the scores (Precision, Recall, and F<sub>1</sub>) provided by scikit's classification_report. It includes the scores per category, as well as micro and macro averages. Discuss whether the scores are balanced or not between the different categories (positive, negative, neutral) and between precision and recall. Discuss the shortcomings (if any) of the classifier based on these scores\n",
    "* an error analysis regarding the misclassifications of the classifier. It involves going through the texts and trying to understand what has gone wrong. It servers to get insight in what could be done to improve the performance of the classifier. Do you observe patterns in misclassifications?  Discuss why these errors are made and propose ways to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Credits\n",
    "The notebooks in this block have been originally created by [Marten Postma](https://martenpostma.github.io) and [Isa Maks](https://research.vu.nl/en/persons/e-maks). Adaptations were made by [Filip Ilievski](http://ilievski.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: VADER assignments\n",
    "\n",
    "\n",
    "### Preparation (nothing to submit):\n",
    "To be able to answer the VADER questions you need to know how the tool works. \n",
    "* Read more about the VADER tool in [this blog](http://t-redactyl.io/blog/2017/04/using-vader-to-handle-sentiment-analysis-with-social-media-text.html).  \n",
    "* VADER provides 4 scores (positive, negative, neutral, compound). Be sure to understand what they mean and how they are calculated.\n",
    "* VADER uses rules to handle linguistic phenomena such as negation and intensification. Be sure to understand which rules are used, how they work, and why they are important.\n",
    "* VADER makes use of a sentiment lexicon. Have a look at the lexicon. Be sure to understand which information can be found there (lemma?, wordform?, part-of-speech?, polarity value?, word meaning?) What do all scores mean? https://github.com/cjhutto/vaderSentiment/blob/master/vaderSentiment/vader_lexicon.txt) \n",
    "\n",
    "\n",
    "### [3.5 points] Question1:\n",
    "\n",
    "Regard the following sentences and their output as given by VADER. Regard sentences 1 to 7, and explain the outcome **for each sentence**. Take into account both the rules applied by VADER and the lexicon that is used. You will find that some of the results are reasonable, but others are not. Explain what is going wrong or not when correct and incorrect results are produced. \n",
    "\n",
    "```\n",
    "INPUT SENTENCE 1 I love apples\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.192, 'pos': 0.808, 'compound': 0.6369}\n",
    "\n",
    "INPUT SENTENCE 2 I don't love apples\n",
    "VADER OUTPUT {'neg': 0.627, 'neu': 0.373, 'pos': 0.0, 'compound': -0.5216}\n",
    "\n",
    "INPUT SENTENCE 3 I love apples :-)\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.133, 'pos': 0.867, 'compound': 0.7579}\n",
    "\n",
    "INPUT SENTENCE 4 These houses are ruins\n",
    "VADER OUTPUT {'neg': 0.492, 'neu': 0.508, 'pos': 0.0, 'compound': -0.4404}\n",
    "\n",
    "INPUT SENTENCE 5 These houses are certainly not considered ruins\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.51, 'pos': 0.49, 'compound': 0.5867}\n",
    "\n",
    "INPUT SENTENCE 6 He lies in the chair in the garden\n",
    "VADER OUTPUT {'neg': 0.286, 'neu': 0.714, 'pos': 0.0, 'compound': -0.4215}\n",
    "\n",
    "INPUT SENTENCE 7 This house is like any house\n",
    "VADER OUTPUT {'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612}\n",
    "```\n",
    "\n",
    "1. Vader correctly identifies the sentence as being highly positive, due to the positive word 'love' being used, and thus gives it a high positive score and a positive compound score.\n",
    "\n",
    "2. Vader sees the negation of 'love' accurately, and thus correctly gives it a negative compound score due to 'don't love' being used in the sentence.\n",
    "\n",
    "3. Vader seems to be able to identify the smiley in the sentence, and also sees the word 'love' being used. Thus the sentence correctly gets a positive compound score, which is even more positive than the first sentence due to the smiley.\n",
    "\n",
    "4. Vader sees the word 'ruins' and correctly identifies this as a negative word, with a high negative score. The negative compound score also reflects the negative sentiment of the sentence accurately.\n",
    "\n",
    "5. Vader accurately assesses the sentiment of the sentence, because the negative word 'ruins' is negated within this sentence with 'certainly not', leading to a positive compound score. The score is also not too high because this sentence is only negating a negative assessment of houses.\n",
    "\n",
    "6. Vader is slightly off with this sentence, probably due to the word 'lies' being used in the sentence, which has multiple meanings, but the negative meaning is not used within this sentence. Vader correctly sees a lot of neutrality in this sentence, but also sees some negativity in the sentence, resulting in the compound score being off. The compound score reflects a pretty negative sentence, whereas this sentence is more of a neutral/lightly positive sentence.\n",
    "\n",
    "7. Vader is also slightly off in the last sentence. The sentence can be both a neutral or negative assessment of a house, because the house doesn't feel distinct from other houses. Vader correctly detects some neutrality in the sentence, due to 'any house' being a mostly neutral description. However, vader also saw the word 'likes', which probably increased the positivity score of the sentence, leading to a somewhat positive compound score. The positive and negative score should be interchanged, and the compound score should be closer to 0 or slightly negative to accurately reflect the slightly negative tone of this sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Points: 2.5] Exercise 2: Collecting 50 tweets for evaluation\n",
    "Collect 50 tweets. Try to find tweets that are interesting for sentiment analysis, e.g., very positive, neutral, and negative tweets. These could be your own tweets (typed in) or collected from the Twitter stream.\n",
    "\n",
    "We will store the tweets in the file **my_tweets.json** (use a text editor to edit).\n",
    "For each tweet, you should insert:\n",
    "* sentiment analysis label: negative | neutral | positive (this you determine yourself, this is not done by a computer)\n",
    "* the text of the tweet\n",
    "* the Tweet-URL\n",
    "\n",
    "from:\n",
    "```\n",
    "    \"1\": {\n",
    "        \"sentiment_label\": \"\",\n",
    "        \"text_of_tweet\": \"\",\n",
    "        \"tweet_url\": \"\",\n",
    "```\n",
    "to:\n",
    "```\n",
    "\"1\": {\n",
    "        \"sentiment_label\": \"positive\",\n",
    "        \"text_of_tweet\": \"All across America people chose to get involved, get engaged and stand up. Each of us can make a difference, and all of us ought to try. So go keep changing the world in 2018.\",\n",
    "        \"tweet_url\" : \"https://twitter.com/BarackObama/status/946775615893655552\",\n",
    "    },\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load your tweets with human annotation in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tweets = json.load(open('my_tweets.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 {'sentiment_label': 'negative', 'text_of_tweet': 'Iâ€™m actually like so angry about Iceland I canâ€™t stop thinking about it. ', 'tweet_url': 'https://twitter.com/MaxxyRainbow/status/1764168102571360681'}\n"
     ]
    }
   ],
   "source": [
    "for id_, tweet_info in my_tweets.items():\n",
    "    print(id_, tweet_info)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [5 points] Question 3:\n",
    "\n",
    "Run VADER on your own tweets (see function **run_vader** from notebook **Lab2-Sentiment-analysis-using-VADER.ipynb**). You can use the code snippet below this explanation as a starting point. \n",
    "* [2.5 points] a. Perform a quantitative evaluation. Explain the different scores, and explain which scores are most relevant and why.\n",
    "* [2.5 points] b. Perform an error analysis: select 10 positive, 10 negative and 10 neutral tweets that are not correctly classified and try to understand why. Refer to the VADER-rules and the VADER-lexicon. Of course, if there are less than 10 errors for a category, you only have to check those. For example, if there are only 5 errors for positive tweets, you just describe those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a.  Looking at the quantitative analysis, we can see that in all three metrics (precision, recall and F1-score) neutral is more often incorrectly classified as compared to the other two categories. This can be explained by the fact that even though a text may be neutral, it can still contain words that have a negative or positive connotation when used in a subjective text. Think with this for example of a factual statement about hurricanes: the text \"a hurricane is a type of natural disaster\" is undoubtedly neutral. However, the word \"disaster\" has a negative pretense, and thus the sentence might be classified as negative. Assuming we are trying to assess the negativity/positivity of the platform, we would consider F1-score to be the most important metric in that regard. This metric most accurately encompasses the goal of trying to get an analysis as close to reality as possible. If for example, we were trying to make automatic moderation automatically banning tweets that are too negative, precision would be more important, since you dont want to ban any innocent people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. \n",
    "neutral: words like disaster, harsh, prison and strange are interpreted as negative words, while in this context they are not used this way. The same goes in the positive direction for words like safety and security can be seen as positive, while in this specific sentence they are used very factually. This problem was described earlier in 3a.\n",
    "\n",
    "negative: For some sentences, even though the sentence is ultimately negative, there is a lot of neutral/lightly positive words mixed in there as well (maybe in a contradiction, or someone hopes for better, or simply in their wording), making the final label different from the actual one. VADER of course cannot pick up on the subtle nuances and contextual clues (something like sarcasm or requiring knowledge about the world) very well, making sentences like sentence #6 hard to interpret. another possibility is VADER misinterpreting the sentence and using the wrong version of a word (like has multiple meanings) for its sentiment analysis. This leads it to incorrectly classifying sentences. \n",
    "Then there are also the sentences that dont contain any positively or negatively connotated words, thus leading to a false label. This is thus simply a lack of completeness in the VADER lexicon.\n",
    "For the sentence about burger king we dont really know why it is classified as positive, since we can only find either a neutral interpretation or negatively annotated words. Still the sentence is classified as lightly positive.\n",
    "\n",
    "positive: In some sentences, some very positive words are not part of the VADER lexicon (incredible being one). Sometimes the lemmatization changes positive words into negative ones (cutest -> cut). In other sentences there is not really one clear label (the madonna tweet was meant ironic; They describe a negative thing with a positive intention). And finally, just like for the negative tweets, sometimes a sentence contains words that on their own are interpreted as the other direction -> mad is very negative, thus the whole sentence is interpreted as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load(\"en_core_web_sm\") # 'en_core_web_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "    input_to_vader = []\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_ if token.lemma_ != '-PRON-' else token.text\n",
    "\n",
    "            if not parts_of_speech_to_consider:\n",
    "                input_to_vader.append(to_add)\n",
    "                continue\n",
    "            if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add)\n",
    "\n",
    "    return vader_model.polarity_scores(' '.join(input_to_vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LABEL: negative V-LABEL: positive TWEET: ffs i did not want to wake up to this israel news maybe ive been too naive but i really did trust the EBU would actually do something\n",
      "LABEL: neutral V-LABEL: negative TWEET: Designed to withstand the harsh environment of Mars, these squishy robots could transform how first responders determine their approach to a disaster scene here on Earth. Learn how  @NASASpinoff tech could help disaster response: https://go.nasa.gov/3IpybN2\n",
      "LABEL: positive V-LABEL: neutral TWEET: Incredible pan across Marsâ€™ surface from NASAâ€™s Perseverance rover ðŸ˜²\n",
      "LABEL: negative V-LABEL: positive TWEET: This is what Mali, Africa looks like today. This is what the future of Western countries looks like when you keep importing the third world. You become the third world.\n",
      "LABEL: negative V-LABEL: neutral TWEET: UK just threw a guy into jail for 2 years for stickers Stickers\n",
      "LABEL: negative V-LABEL: positive TWEET: Something how Dr. Fauci is revered by the LameStream Media as such a great professional, having done, they say, such an incredible job, yet he works for me and the Trump Administration, and I am in no way given any credit for my work. Gee, could this just be more Fake News?\n",
      "LABEL: negative V-LABEL: neutral TWEET: The biggest lie the legacy media makes is narrative: choosing what to write about and what not to write about\n",
      "LABEL: positive V-LABEL: negative TWEET: Red Pandas are the cutest creatures on earth\n",
      "LABEL: positive V-LABEL: negative TWEET: 35 years ago madonna released â€˜like a prayerâ€™ and the vatican banned it, iconic.\n",
      "LABEL: positive V-LABEL: neutral TWEET: my brother got my name tatted with a lil heart next to it ðŸ¥²ðŸ¥²\n",
      "LABEL: neutral V-LABEL: positive TWEET: Testing the safety & security on an inflatable car storage by WhistlinDiesel. I mean, could they have thrown anything else?\n",
      "LABEL: neutral V-LABEL: negative TWEET: The US threw a dude in prison over a meme  We live in strange times\n",
      "LABEL: negative V-LABEL: positive TWEET: This is a receipt from Burger King in 1986.  3 whoppers, 2 fries and 2 large vanilla ice cream for $8.39         Never forget what they took from us ðŸ˜°\n",
      "LABEL: positive V-LABEL: negative TWEET: I donâ€™t have beef with a single soul . If ur mad get well soon\n",
      "LABEL: negative V-LABEL: positive TWEET: It's laughable that anyone, male or female, that looks like this is permitted to make health decisions for a country.\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.71      0.63      0.67        19\n",
      "     neutral       0.43      0.50      0.46         6\n",
      "    positive       0.77      0.80      0.78        25\n",
      "\n",
      "    accuracy                           0.70        50\n",
      "   macro avg       0.63      0.64      0.64        50\n",
      "weighted avg       0.70      0.70      0.70        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "all_vader_output = []\n",
    "gold = []\n",
    "\n",
    "# settings (to change for different experiments)\n",
    "to_lemmatize = True \n",
    "pos = set()\n",
    "\n",
    "for id_, tweet_info in my_tweets.items():\n",
    "    the_tweet = tweet_info['text_of_tweet']\n",
    "    vader_output = run_vader(the_tweet, to_lemmatize, pos)\n",
    "    vader_label = vader_output_to_label(vader_output)\n",
    "    \n",
    "    tweets.append(the_tweet)\n",
    "    all_vader_output.append(vader_label)\n",
    "    gold.append(tweet_info['sentiment_label'])\n",
    "    \n",
    "    if vader_label != tweet_info['sentiment_label']:\n",
    "        print(f\"LABEL: {tweet_info['sentiment_label']} V-LABEL: {vader_label} TWEET: {the_tweet}\")\n",
    "    \n",
    "# use scikit-learn's classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true=gold, y_pred=all_vader_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 4:\n",
    "Run VADER on the set of airline tweets with the following settings:\n",
    "\n",
    "* Run VADER (as it is) on the set of airline tweets \n",
    "* Run VADER on the set of airline tweets after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only adjectives\n",
    "* Run VADER on the set of airline tweets with only adjectives and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only nouns\n",
    "* Run VADER on the set of airline tweets with only nouns and after having lemmatized the text\n",
    "* Run VADER on the set of airline tweets with only verbs\n",
    "* Run VADER on the set of airline tweets with only verbs and after having lemmatized the text\n",
    "\n",
    "* [1 point] a. Generate for all separate experiments the classification report, i.e., Precision, Recall, and F<sub>1</sub> scores per category as well as micro and macro averages. **Use a different code cell (or multiple code cells) for each experiment.**\n",
    "* [3 points] b. Compare the scores and explain what they tell you.\n",
    "* - Does lemmatisation help? Explain why or why not.\n",
    "* - Are all parts of speech equally important for sentiment analysis? Explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. \n",
    "* Lemmatisation does not seem to have a significant effect on the outcome of the algorithms results. This is probably due to the fact that most lemmatized words dont change sentiment from their original form. In the end, the judgement stays relatively the same (loving -> love doesnt suddenly become negative)\n",
    "* As far as we can tell, the order from most important to least goes as follows: Adjectives -> Verbs -> Nouns. This is most likely due to the fact that adjectives are used to describe something, and depending on the choice of specific adjective you can choose what sentiment to convey (bad apple vs good apple). The noun is simply the subject of the sentence, and whether you have a positive or a bad opinion doesnt change the fact that you use them. As for verbs, there is a little bit of variation you can make depending on sentiment, but oftentimes the verb just describes a certain action, which would be described anyways as well. Some verbs can however have a sentiment of themselves (loving, hating, killing, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: d:\\school\\TUDelft\\Python\\Artificial Intelligence\\Text Mining\\ba-text-mining-group19\\lab_sessions\\lab3\\airlinetweets\n",
      "this will print True if the folder exists: True\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "cwd = pathlib.Path.cwd()\n",
    "airline_tweets_folder = cwd.joinpath('airlinetweets')\n",
    "print('path:', airline_tweets_folder)\n",
    "print('this will print True if the folder exists:', \n",
    "    airline_tweets_folder.exists())\n",
    "\n",
    "# loading all files as training data.\n",
    "str(airline_tweets_folder)\n",
    "airline_tweets_train = load_files(str(airline_tweets_folder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4755\n"
     ]
    }
   ],
   "source": [
    "print(len(airline_tweets_train.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the zip file \"airlinetweets.zip\" and extract the file \"airline_tweets.json\n",
    "\n",
    "def run_vader_on_airline_tweets(to_lemmatize: bool, pos: set):\n",
    "    gold = []\n",
    "    all_vader_output = []   \n",
    "    for tweet in airline_tweets_train.data:\n",
    "        tweet = tweet.decode('utf-8')\n",
    "        vader_output = run_vader(tweet, to_lemmatize, pos)\n",
    "        vader_label = vader_output_to_label(vader_output)\n",
    "\n",
    "        all_vader_output.append(vader_label)\n",
    "\n",
    "    for target in airline_tweets_train.target:\n",
    "        gold.append(airline_tweets_train.target_names[target])\n",
    "\n",
    "    #print the classification report\n",
    "    print(classification_report(y_true=gold, y_pred=all_vader_output))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As is\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.80      0.51      0.63      1750\n",
      "     neutral       0.60      0.51      0.55      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.63      4755\n",
      "   macro avg       0.65      0.64      0.62      4755\n",
      "weighted avg       0.66      0.63      0.62      4755\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#run vader on airline tweets\n",
    "print('As is')\n",
    "run_vader_on_airline_tweets(to_lemmatize=False, pos=None)\n",
    "print(\"______________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmatized\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.52      0.63      1750\n",
      "     neutral       0.60      0.49      0.54      1515\n",
      "    positive       0.56      0.88      0.68      1490\n",
      "\n",
      "    accuracy                           0.62      4755\n",
      "   macro avg       0.65      0.63      0.62      4755\n",
      "weighted avg       0.65      0.62      0.62      4755\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 2. lemmatized\n",
    "print('lemmatized')\n",
    "run_vader_on_airline_tweets(to_lemmatize=True, pos=None)\n",
    "print(\"______________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only adjectives\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.21      0.34      1750\n",
      "     neutral       0.40      0.89      0.56      1515\n",
      "    positive       0.66      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.51      0.47      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 3. adjectives\n",
    "print(\"Only adjectives\")\n",
    "run_vader_on_airline_tweets(to_lemmatize=False, pos={'ADJ'})\n",
    "print(\"______________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only adjectives & lemmatized\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.21      0.34      1750\n",
      "     neutral       0.40      0.89      0.56      1515\n",
      "    positive       0.66      0.44      0.53      1490\n",
      "\n",
      "    accuracy                           0.50      4755\n",
      "   macro avg       0.65      0.51      0.47      4755\n",
      "weighted avg       0.66      0.50      0.47      4755\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 4. adjectives & lemmatized\n",
    "print(\"Only adjectives & lemmatized\")\n",
    "run_vader_on_airline_tweets(to_lemmatize=True, pos={'ADJ'})\n",
    "print(\"______________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only nouns\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.14      0.24      1750\n",
      "     neutral       0.36      0.82      0.50      1515\n",
      "    positive       0.53      0.34      0.41      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.54      0.43      0.38      4755\n",
      "weighted avg       0.55      0.42      0.38      4755\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 5. nouns\n",
    "print(\"Only nouns\")\n",
    "run_vader_on_airline_tweets(to_lemmatize=False, pos={'NOUN'})\n",
    "print(\"______________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only nouns & lemmatized\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.16      0.26      1750\n",
      "     neutral       0.36      0.81      0.50      1515\n",
      "    positive       0.52      0.33      0.40      1490\n",
      "\n",
      "    accuracy                           0.42      4755\n",
      "   macro avg       0.53      0.43      0.39      4755\n",
      "weighted avg       0.54      0.42      0.38      4755\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 6. nouns & lemmatized\n",
    "print(\"Only nouns & lemmatized\")\n",
    "run_vader_on_airline_tweets(to_lemmatize=True, pos={'NOUN'})\n",
    "print(\"______________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only verbs\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.77      0.29      0.42      1750\n",
      "     neutral       0.38      0.81      0.52      1515\n",
      "    positive       0.57      0.34      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.58      0.48      0.46      4755\n",
      "weighted avg       0.59      0.47      0.45      4755\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 7. verbs\n",
    "print(\"Only verbs\")\n",
    "run_vader_on_airline_tweets(to_lemmatize=False, pos={'VERB'})\n",
    "print(\"______________________________________________________________________________________\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only verbs & lemmatized\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.74      0.30      0.42      1750\n",
      "     neutral       0.38      0.78      0.51      1515\n",
      "    positive       0.57      0.35      0.43      1490\n",
      "\n",
      "    accuracy                           0.47      4755\n",
      "   macro avg       0.56      0.48      0.46      4755\n",
      "weighted avg       0.57      0.47      0.45      4755\n",
      "\n",
      "______________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 8. verbs & lemmatized\n",
    "print(\"Only verbs & lemmatized\")\n",
    "run_vader_on_airline_tweets(to_lemmatize=True, pos={'VERB'})\n",
    "print(\"______________________________________________________________________________________\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: scikit-learn assignments\n",
    "### [4 points] Question 5\n",
    "Train the scikit-learn classifier (Naive Bayes) using the airline tweets.\n",
    "\n",
    "+ Train the model on the airline tweets with 80% training and 20% test set and default settings (TF-IDF representation, min_df=2)\n",
    "+ Train with different settings:\n",
    "    + with respect to vectorizing: TF-IDF ('airline_tfidf') vs. Bag of words representation ('airline_count') \n",
    "    + with respect to the frequency threshold (min_df). Carry out experiments with increasing values for document frequency (min_df = 2; min_df = 5; min_df =10) \n",
    "* [1 point] a. Generate a classification_report for all experiments\n",
    "* [3 points] b. Look at the results of the experiments with the different settings and try to explain why they differ: \n",
    "    + which category performs best, is this the case for any setting?\n",
    "    + does the frequency threshold affect the scores? Why or why not according to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.\n",
    "* Just like in question 4, the difference between td-idf and bag of words representation doesnt seem to differ significantly enough to infer either being superior to the other. This goes for any of the min_df setting, although the small differences that do occur do differ between the settings.\n",
    "* between a min_df of 2 and 5 there is very little noticeable difference. however, between 5 and 10 there is a larger difference. This is likely because the threshold of 10 is much harsher than that of 5, and will thus exclude much more words from consideration. This seems to, in this case, affect negative words the most in terms of precision and neutral the most in terms of recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.92      0.86       337\n",
      "     neutral       0.85      0.70      0.77       323\n",
      "    positive       0.83      0.87      0.85       291\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.83      0.83       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                            tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                            stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "#generate a classification report\n",
    "print(classification_report(y_test, y_pred, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.85      0.90      0.87       353\n",
      "     neutral       0.84      0.74      0.79       308\n",
      "    positive       0.83      0.88      0.85       290\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.84      0.84      0.84       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=2, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                            tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                            stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "#generate a classification report\n",
    "print(classification_report(y_test, y_pred, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.90      0.86       346\n",
      "     neutral       0.87      0.73      0.80       313\n",
      "    positive       0.85      0.89      0.87       292\n",
      "\n",
      "    accuracy                           0.84       951\n",
      "   macro avg       0.85      0.84      0.84       951\n",
      "weighted avg       0.84      0.84      0.84       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=5, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                            tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                            stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "#generate a classification report\n",
    "print(classification_report(y_test, y_pred, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.90      0.87       365\n",
      "     neutral       0.79      0.74      0.76       288\n",
      "    positive       0.83      0.81      0.82       298\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.82      0.82      0.82       951\n",
      "weighted avg       0.82      0.83      0.82       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=5, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                            tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                            stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "#generate a classification report\n",
    "print(classification_report(y_test, y_pred, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.79      0.90      0.84       326\n",
      "     neutral       0.81      0.70      0.75       313\n",
      "    positive       0.83      0.82      0.83       312\n",
      "\n",
      "    accuracy                           0.81       951\n",
      "   macro avg       0.81      0.81      0.81       951\n",
      "weighted avg       0.81      0.81      0.81       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=10, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                            tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                            stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_tfidf, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "#generate a classification report\n",
    "print(classification_report(y_test, y_pred, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\Kelvin Brachthuizen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.93      0.88       372\n",
      "     neutral       0.82      0.71      0.76       295\n",
      "    positive       0.84      0.82      0.83       284\n",
      "\n",
      "    accuracy                           0.83       951\n",
      "   macro avg       0.83      0.82      0.82       951\n",
      "weighted avg       0.83      0.83      0.83       951\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airline_vec = CountVectorizer(min_df=10, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                            tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                            stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "airline_counts = airline_vec.fit_transform(airline_tweets_train.data)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "airline_tfidf = tfidf_transformer.fit_transform(airline_counts)\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(\n",
    "    airline_counts, # the tf-idf model\n",
    "    airline_tweets_train.target, # the category values for each tweet \n",
    "    test_size = 0.20 # we use 80% for training and 20% for development\n",
    "    ) \n",
    "\n",
    "clf = MultinomialNB().fit(docs_train, y_train)\n",
    "y_pred = clf.predict(docs_test)\n",
    "\n",
    "#generate a classification report\n",
    "print(classification_report(y_test, y_pred, target_names=airline_tweets_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [4 points] Question 6: Inspecting the best scoring features \n",
    "\n",
    "+ Train the scikit-learn classifier (Naive Bayes) model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "* [1 point] a. Generate the list of best scoring features per class (see function **important_features_per_class** below) [1 point]\n",
    "* [3 points] b. Look at the lists and consider the following issues: \n",
    "    + [1 point] Which features did you expect for each separate class and why?\n",
    "    + [1 point] Which features did you not expect and why ? \n",
    "    + [1 point] The list contains all kinds of words such as names of airlines, punctuation, numbers and content words (e.g., 'delay' and 'bad'). Which words would you remove or keep when trying to improve the model and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def important_features_per_class(vectorizer,classifier,n=80):\n",
    "    class_labels = classifier.classes_\n",
    "    feature_names =vectorizer.get_feature_names()\n",
    "    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n",
    "    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n",
    "    topn_class3 = sorted(zip(classifier.feature_count_[2], feature_names),reverse=True)[:n]\n",
    "    print(\"Important words in negative documents\")\n",
    "    for coef, feat in topn_class1:\n",
    "        print(class_labels[0], coef, feat)\n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in neutral documents\")\n",
    "    for coef, feat in topn_class2:\n",
    "        print(class_labels[1], coef, feat) \n",
    "    print(\"-----------------------------------------\")\n",
    "    print(\"Important words in positive documents\")\n",
    "    for coef, feat in topn_class3:\n",
    "        print(class_labels[2], coef, feat) \n",
    "\n",
    "# example of how to call from notebook:\n",
    "#important_features_per_class(airline_vec, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not  be graded)] Question 7\n",
    "Train the model on airline tweets and test it on your own set of tweets\n",
    "+ Train the model with the following settings (airline tweets 80% training and 20% test;  Bag of words representation ('airline_count'), min_df=2)\n",
    "+ Apply the model on your own set of tweets and generate the classification report\n",
    "* [1 point] a. Carry out a quantitative analysis.\n",
    "* [1 point] b. Carry out an error analysis on 10 correctly and 10 incorrectly classified tweets and discuss them\n",
    "* [2 points] c. Compare the results (cf. classification report) with the results obtained by VADER on the same tweets and discuss the differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Optional! (will not be graded)] Question 8: trying to improve the model\n",
    "* [2 points] a. Think of some ways to improve the scikit-learn Naive Bayes model by playing with the settings or applying linguistic preprocessing (e.g., by filtering on part-of-speech, or removing punctuation). Do not change the classifier but continue using the Naive Bayes classifier. Explain what the effects might be of these other settings \n",
    "+ [1 point] b. Apply the model with at least one new setting (train on the airline tweets using 80% training, 20% test) and generate the scores\n",
    "* [1 point] c. Discuss whether the model achieved what you expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End of this notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
