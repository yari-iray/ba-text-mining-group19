{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "from datasets import load_dataset\n",
    "import json\n",
    "\n",
    "dataset = load_dataset(\"tner/ontonotes5\")\n",
    "ontonotes5_train_dataset = pd.DataFrame(dataset['train'])\n",
    "\n",
    "# Read the JSON file\n",
    "with open('dataset_label.json', 'r') as file:\n",
    "    labels = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('People', 'O', 'NNS'), ('start', 'O', 'NN'), ('their', 'O', 'PRP$'), ('own', 'O', 'JJ'), ('businesses', 'O', 'NNS'), ('for', 'O', 'IN'), ('many', 'O', 'JJ'), ('reasons', 'O', 'NNS'), ('.', 'O', '.'), ('But', 'O', 'CC')]\n"
     ]
    }
   ],
   "source": [
    "labels = { v:k for k,v in labels.items() }\n",
    "\n",
    "training_tokens = []\n",
    "\n",
    "for row in ontonotes5_train_dataset.iterrows():\n",
    "    zipped = list(zip(row[1]['tokens'], row[1]['tags']))\n",
    "    \n",
    "    for item in zipped:\n",
    "        training_tokens.append((item[0], labels[item[1]], nltk.pos_tag([item[0]])[0][1]))\n",
    "\n",
    "print(training_tokens[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence id</th>\n",
       "      <th>token id</th>\n",
       "      <th>token</th>\n",
       "      <th>BIO NER tag</th>\n",
       "      <th>POS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>I</td>\n",
       "      <td>O</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>would</td>\n",
       "      <td>O</td>\n",
       "      <td>MD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>n't</td>\n",
       "      <td>O</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>be</td>\n",
       "      <td>O</td>\n",
       "      <td>VB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>caught</td>\n",
       "      <td>O</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>dead</td>\n",
       "      <td>O</td>\n",
       "      <td>JJ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>watching</td>\n",
       "      <td>O</td>\n",
       "      <td>VBG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "      <td>DT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>NFL</td>\n",
       "      <td>B-ORG</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>if</td>\n",
       "      <td>O</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>it</td>\n",
       "      <td>O</td>\n",
       "      <td>PRP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>were</td>\n",
       "      <td>O</td>\n",
       "      <td>VBD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>n't</td>\n",
       "      <td>O</td>\n",
       "      <td>RB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>for</td>\n",
       "      <td>O</td>\n",
       "      <td>IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>Taylor</td>\n",
       "      <td>B-PERSON</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>Swift</td>\n",
       "      <td>I-PERSON</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Chris</td>\n",
       "      <td>B-PERSON</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>O'Donnell</td>\n",
       "      <td>I-PERSON</td>\n",
       "      <td>NN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>stated</td>\n",
       "      <td>O</td>\n",
       "      <td>VBN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence id  token id      token BIO NER tag  POS\n",
       "0             0         0          I           O  PRP\n",
       "1             0         1      would           O   MD\n",
       "2             0         2        n't           O   RB\n",
       "3             0         3         be           O   VB\n",
       "4             0         4     caught           O   NN\n",
       "5             0         5       dead           O   JJ\n",
       "6             0         6   watching           O  VBG\n",
       "7             0         7        the           O   DT\n",
       "8             0         8        NFL       B-ORG   NN\n",
       "9             0         9         if           O   IN\n",
       "10            0        10         it           O  PRP\n",
       "11            0        11       were           O  VBD\n",
       "12            0        12        n't           O   RB\n",
       "13            0        13        for           O   IN\n",
       "14            0        14     Taylor    B-PERSON   NN\n",
       "15            0        15      Swift    I-PERSON   NN\n",
       "16            0        16          .           O    .\n",
       "17            1         0      Chris    B-PERSON   NN\n",
       "18            1         1  O'Donnell    I-PERSON   NN\n",
       "19            1         2     stated           O  VBN"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ner_test = pd.read_csv('data/ner-test.tsv', delimiter='\\t')\n",
    "\n",
    "# adding POS tags\n",
    "def pos_tagging(token):\n",
    "    return nltk.pos_tag([token])[0][1]\n",
    "\n",
    "df_ner_test['POS'] = df_ner_test['token'].apply(pos_tagging)\n",
    "df_ner_test.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Added the NLTK pos_tags to the test set because they are a useful feature for NERC.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = []\n",
    "test_gold_labels = []\n",
    "\n",
    "test_words = df_ner_test.iterrows()\n",
    "for i, row in test_words:\n",
    "    token: str = row['token']\n",
    "    \n",
    "    a_dict = {\n",
    "        'word': token,                # the word itself\n",
    "        'pos': row['POS'],                   # the part of speech\n",
    "        'word[-3:]': token[-3:],      # the last three characters of the word\n",
    "        'word[-2:]': token[-2:],      # the last two characters of the word\n",
    "        'upper': token[0].isupper(),  # whether the first letter is uppercase\n",
    "        'title': token.istitle(),      # whether the word is titlecased \n",
    "        'prev-word': \"!NEWSENTENCE\"     # We had None type if the first word but that screwed with the \n",
    "                                 \n",
    "        # 'upper' and 'lower' will be slightly different because\n",
    "        # of words like 'EU'\n",
    "    }\n",
    "    \n",
    "    #If we have a previous word, add it to the features list to improve NERC\n",
    "    if i > 0:\n",
    "        # only get previous word if they're part of the same sentence\n",
    "        prev_row = df_ner_test.iloc[i - 1]\n",
    "        \n",
    "        if prev_row['sentence id'] == row['sentence id']:\n",
    "            a_dict['prev-word'] = prev_row['token']\n",
    "\n",
    "    test_features.append(a_dict)\n",
    "    test_gold_labels.append(row['BIO NER tag'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'word': 'I',\n",
       "  'pos': 'PRP',\n",
       "  'word[-3:]': 'I',\n",
       "  'word[-2:]': 'I',\n",
       "  'upper': True,\n",
       "  'title': True,\n",
       "  'prev-word': '!NEWSENTENCE'},\n",
       " {'word': 'would',\n",
       "  'pos': 'MD',\n",
       "  'word[-3:]': 'uld',\n",
       "  'word[-2:]': 'ld',\n",
       "  'upper': False,\n",
       "  'title': False,\n",
       "  'prev-word': 'I'},\n",
       " {'word': \"n't\",\n",
       "  'pos': 'RB',\n",
       "  'word[-3:]': \"n't\",\n",
       "  'word[-2:]': \"'t\",\n",
       "  'upper': False,\n",
       "  'title': False,\n",
       "  'prev-word': 'would'},\n",
       " {'word': 'be',\n",
       "  'pos': 'VB',\n",
       "  'word[-3:]': 'be',\n",
       "  'word[-2:]': 'be',\n",
       "  'upper': False,\n",
       "  'title': False,\n",
       "  'prev-word': \"n't\"},\n",
       " {'word': 'caught',\n",
       "  'pos': 'NN',\n",
       "  'word[-3:]': 'ght',\n",
       "  'word[-2:]': 'ht',\n",
       "  'upper': False,\n",
       "  'title': False,\n",
       "  'prev-word': 'be'},\n",
       " {'word': 'dead',\n",
       "  'pos': 'JJ',\n",
       "  'word[-3:]': 'ead',\n",
       "  'word[-2:]': 'ad',\n",
       "  'upper': False,\n",
       "  'title': False,\n",
       "  'prev-word': 'caught'},\n",
       " {'word': 'watching',\n",
       "  'pos': 'VBG',\n",
       "  'word[-3:]': 'ing',\n",
       "  'word[-2:]': 'ng',\n",
       "  'upper': False,\n",
       "  'title': False,\n",
       "  'prev-word': 'dead'},\n",
       " {'word': 'the',\n",
       "  'pos': 'DT',\n",
       "  'word[-3:]': 'the',\n",
       "  'word[-2:]': 'he',\n",
       "  'upper': False,\n",
       "  'title': False,\n",
       "  'prev-word': 'watching'},\n",
       " {'word': 'NFL',\n",
       "  'pos': 'NN',\n",
       "  'word[-3:]': 'NFL',\n",
       "  'word[-2:]': 'FL',\n",
       "  'upper': True,\n",
       "  'title': False,\n",
       "  'prev-word': 'the'},\n",
       " {'word': 'if',\n",
       "  'pos': 'IN',\n",
       "  'word[-3:]': 'if',\n",
       "  'word[-2:]': 'if',\n",
       "  'upper': False,\n",
       "  'title': False,\n",
       "  'prev-word': 'NFL'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus.reader import ConllCorpusReader\n",
    "# ### Adapt the path to point to the CONLL2003 folder on your local machine\n",
    "# train = ConllCorpusReader('./data/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "\n",
    "# train_words = list(train.iob_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_features = []\n",
    "training_gold_labels = []\n",
    "\n",
    "for i, (token, ne_label, pos) in enumerate(training_tokens):\n",
    "   if token == '' or token == 'DOCSTART':\n",
    "      continue\n",
    "   \n",
    "   a_dict = {\n",
    "      'word': token,                # the word itself\n",
    "      'pos': pos,                   # the part of speech\n",
    "      'word[-3:]': token[-3:],      # the last three characters of the word\n",
    "      'word[-2:]': token[-2:],      # the last two characters of the word\n",
    "      'upper': token[0].isupper(),  # whether the first letter is uppercase\n",
    "      'title': token.istitle(),     # whether the word is titlecased \n",
    "      'prev-word': \"!NEWSENTENCE\",       # the previous word (!NEWSENTENCE if it's the first word)\n",
    "   }\n",
    "   \n",
    "   # If the sentence is not ending\n",
    "   if i > 0:\n",
    "      prev_token = training_tokens[i - 1][0]\n",
    "      if prev_token != '.':\n",
    "         a_dict['prev-word'] = prev_token\n",
    "      \n",
    "\n",
    "   training_features.append(a_dict)\n",
    "   training_gold_labels.append(ne_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'word': 'People', 'pos': 'NNS', 'word[-3:]': 'ple', 'word[-2:]': 'le', 'upper': True, 'title': True, 'prev-word': '!NEWSENTENCE'}, {'word': 'start', 'pos': 'NN', 'word[-3:]': 'art', 'word[-2:]': 'rt', 'upper': False, 'title': False, 'prev-word': 'People'}, {'word': 'their', 'pos': 'PRP$', 'word[-3:]': 'eir', 'word[-2:]': 'ir', 'upper': False, 'title': False, 'prev-word': 'start'}, {'word': 'own', 'pos': 'JJ', 'word[-3:]': 'own', 'word[-2:]': 'wn', 'upper': False, 'title': False, 'prev-word': 'their'}, {'word': 'businesses', 'pos': 'NNS', 'word[-3:]': 'ses', 'word[-2:]': 'es', 'upper': False, 'title': False, 'prev-word': 'own'}, {'word': 'for', 'pos': 'IN', 'word[-3:]': 'for', 'word[-2:]': 'or', 'upper': False, 'title': False, 'prev-word': 'businesses'}, {'word': 'many', 'pos': 'JJ', 'word[-3:]': 'any', 'word[-2:]': 'ny', 'upper': False, 'title': False, 'prev-word': 'for'}, {'word': 'reasons', 'pos': 'NNS', 'word[-3:]': 'ons', 'word[-2:]': 'ns', 'upper': False, 'title': False, 'prev-word': 'many'}, {'word': '.', 'pos': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'upper': False, 'title': False, 'prev-word': 'reasons'}, {'word': 'But', 'pos': 'CC', 'word[-3:]': 'But', 'word[-2:]': 'ut', 'upper': True, 'title': True, 'prev-word': '!NEWSENTENCE'}, {'word': 'a', 'pos': 'DT', 'word[-3:]': 'a', 'word[-2:]': 'a', 'upper': False, 'title': False, 'prev-word': 'But'}, {'word': 'chance', 'pos': 'NN', 'word[-3:]': 'nce', 'word[-2:]': 'ce', 'upper': False, 'title': False, 'prev-word': 'a'}, {'word': 'to', 'pos': 'TO', 'word[-3:]': 'to', 'word[-2:]': 'to', 'upper': False, 'title': False, 'prev-word': 'chance'}, {'word': 'fill', 'pos': 'NN', 'word[-3:]': 'ill', 'word[-2:]': 'll', 'upper': False, 'title': False, 'prev-word': 'to'}, {'word': 'out', 'pos': 'IN', 'word[-3:]': 'out', 'word[-2:]': 'ut', 'upper': False, 'title': False, 'prev-word': 'fill'}, {'word': 'sales', 'pos': 'NNS', 'word[-3:]': 'les', 'word[-2:]': 'es', 'upper': False, 'title': False, 'prev-word': 'out'}, {'word': '-', 'pos': ':', 'word[-3:]': '-', 'word[-2:]': '-', 'upper': False, 'title': False, 'prev-word': 'sales'}, {'word': 'tax', 'pos': 'NN', 'word[-3:]': 'tax', 'word[-2:]': 'ax', 'upper': False, 'title': False, 'prev-word': '-'}, {'word': 'records', 'pos': 'NNS', 'word[-3:]': 'rds', 'word[-2:]': 'ds', 'upper': False, 'title': False, 'prev-word': 'tax'}, {'word': 'is', 'pos': 'VBZ', 'word[-3:]': 'is', 'word[-2:]': 'is', 'upper': False, 'title': False, 'prev-word': 'records'}, {'word': 'rarely', 'pos': 'RB', 'word[-3:]': 'ely', 'word[-2:]': 'ly', 'upper': False, 'title': False, 'prev-word': 'is'}, {'word': 'one', 'pos': 'CD', 'word[-3:]': 'one', 'word[-2:]': 'ne', 'upper': False, 'title': False, 'prev-word': 'rarely'}, {'word': 'of', 'pos': 'IN', 'word[-3:]': 'of', 'word[-2:]': 'of', 'upper': False, 'title': False, 'prev-word': 'one'}, {'word': 'them', 'pos': 'PRP', 'word[-3:]': 'hem', 'word[-2:]': 'em', 'upper': False, 'title': False, 'prev-word': 'of'}, {'word': '.', 'pos': '.', 'word[-3:]': '.', 'word[-2:]': '.', 'upper': False, 'title': False, 'prev-word': 'them'}, {'word': 'Red', 'pos': 'JJ', 'word[-3:]': 'Red', 'word[-2:]': 'ed', 'upper': True, 'title': True, 'prev-word': '!NEWSENTENCE'}, {'word': 'tape', 'pos': 'NN', 'word[-3:]': 'ape', 'word[-2:]': 'pe', 'upper': False, 'title': False, 'prev-word': 'Red'}, {'word': 'is', 'pos': 'VBZ', 'word[-3:]': 'is', 'word[-2:]': 'is', 'upper': False, 'title': False, 'prev-word': 'tape'}, {'word': 'the', 'pos': 'DT', 'word[-3:]': 'the', 'word[-2:]': 'he', 'upper': False, 'title': False, 'prev-word': 'is'}, {'word': 'bugaboo', 'pos': 'NN', 'word[-3:]': 'boo', 'word[-2:]': 'oo', 'upper': False, 'title': False, 'prev-word': 'the'}]\n"
     ]
    }
   ],
   "source": [
    "print(training_features[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 939111,\n",
       "         'I-ORG': 18246,\n",
       "         'B-PERSON': 15429,\n",
       "         'B-GPE': 15405,\n",
       "         'I-DATE': 13333,\n",
       "         'B-ORG': 12820,\n",
       "         'I-PERSON': 11147,\n",
       "         'B-DATE': 10922,\n",
       "         'B-CARDINAL': 7355,\n",
       "         'B-NORP': 6870,\n",
       "         'I-MONEY': 4912,\n",
       "         'I-GPE': 3679,\n",
       "         'I-PERCENT': 2498,\n",
       "         'B-MONEY': 2411,\n",
       "         'I-WORK_OF_ART': 2400,\n",
       "         'I-CARDINAL': 2289,\n",
       "         'B-PERCENT': 1763,\n",
       "         'B-ORDINAL': 1640,\n",
       "         'I-EVENT': 1605,\n",
       "         'B-LOC': 1514,\n",
       "         'I-TIME': 1507,\n",
       "         'I-FAC': 1467,\n",
       "         'I-LOC': 1395,\n",
       "         'I-QUANTITY': 1235,\n",
       "         'B-TIME': 1233,\n",
       "         'B-WORK_OF_ART': 974,\n",
       "         'B-FAC': 860,\n",
       "         'I-LAW': 785,\n",
       "         'B-EVENT': 748,\n",
       "         'B-QUANTITY': 657,\n",
       "         'B-PRODUCT': 606,\n",
       "         'I-PRODUCT': 576,\n",
       "         'I-NORP': 446,\n",
       "         'B-LANGUAGE': 304,\n",
       "         'B-LAW': 282,\n",
       "         'I-LANGUAGE': 13,\n",
       "         'I-ORDINAL': 5})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "Counter(training_tokens[i][1] for i in range(len(training_tokens)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'O': 160,\n",
       "         'I-WORK_OF_ART': 9,\n",
       "         'B-PERSON': 6,\n",
       "         'I-ORG': 6,\n",
       "         'B-WORK_OF_ART': 4,\n",
       "         'B-ORG': 3,\n",
       "         'I-PERSON': 3,\n",
       "         'B-DATE': 1,\n",
       "         'I-DATE': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(df_ner_test['BIO NER tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #count nans in training_features\n",
    "# for key in training_features[0].keys():\n",
    "#     print(key, sum(1 for i in training_features if i[key] is None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(193, 1088442)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_features), len(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1088635, 89833)\n",
      "(1088442, 89833) (193, 89833)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Concat all features\n",
    "all_features = training_features.copy()\n",
    "all_features.extend(test_features)\n",
    "\n",
    "vec = DictVectorizer()\n",
    "the_array = vec.fit_transform(all_features)\n",
    "print(the_array.shape)\n",
    "\n",
    "vec_training_features = the_array[:len(training_features)]\n",
    "vec_test_features = the_array[len(training_features):]\n",
    "\n",
    "print(vec_training_features.shape, vec_test_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/svm/_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   B-CARDINAL       0.00      0.00      0.00         0\n",
      "       B-DATE       1.00      1.00      1.00         1\n",
      "        B-GPE       0.00      0.00      0.00         0\n",
      "        B-ORG       0.67      0.67      0.67         3\n",
      "     B-PERSON       0.86      1.00      0.92         6\n",
      "B-WORK_OF_ART       0.00      0.00      0.00         4\n",
      "       I-DATE       0.50      1.00      0.67         1\n",
      "      I-EVENT       0.00      0.00      0.00         0\n",
      "        I-GPE       0.00      0.00      0.00         0\n",
      "        I-ORG       0.50      0.17      0.25         6\n",
      "     I-PERSON       1.00      0.33      0.50         3\n",
      "I-WORK_OF_ART       0.00      0.00      0.00         9\n",
      "            O       0.92      0.99      0.96       160\n",
      "\n",
      "     accuracy                           0.89       193\n",
      "    macro avg       0.42      0.40      0.38       193\n",
      " weighted avg       0.84      0.89      0.86       193\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "lin_clf = svm.LinearSVC()\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "lin_clf.fit(vec_training_features, training_gold_labels)\n",
    "predicted = lin_clf.predict(vec_test_features)\n",
    "\n",
    "report = classification_report(test_gold_labels, predicted)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "path = \"C:\\\\Users\\\\Yari\\\\Downloads\\\\GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "word_embedding_model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def get_train_vectors_labels() -> tuple[list, list]:\n",
    "    input_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    for token, _, ne_label in training_tokens:\n",
    "        if token =='' or token == 'DOCSTART':\n",
    "            continue\n",
    "        \n",
    "        if token in word_embedding_model:\n",
    "            vector = word_embedding_model[token]\n",
    "        else:\n",
    "            vector = [0]*300\n",
    "            \n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)\n",
    "        \n",
    "    return (input_vectors, labels)\n",
    "\n",
    "def get_vectors_labels(data: pd.DataFrame) -> tuple[list, list]:\n",
    "    input_vectors = []\n",
    "    labels = []\n",
    "    \n",
    "    for _, row in data.iterrows():\n",
    "        token = row['token']\n",
    "        ne_label = row['BIO NER tag']\n",
    "\n",
    "        if token in word_embedding_model:\n",
    "            vector = word_embedding_model[token]\n",
    "        else:\n",
    "            vector = [0] * 300\n",
    "        input_vectors.append(vector)\n",
    "        labels.append(ne_label)\n",
    "            \n",
    "    return (input_vectors, labels)\n",
    "\n",
    "input_vectors, labels = get_train_vectors_labels()\n",
    "test_input_vectors, test_labels = get_vectors_labels(df_ner_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "lin_clf = svm.LinearSVC()\n",
    "lin_clf.fit(input_vectors, labels)\n",
    "predicted_labels = lin_clf.predict(test_input_vectors)\n",
    "\n",
    "report = classification_report(test_labels, predicted_labels)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
