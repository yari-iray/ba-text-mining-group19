{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import nltk\n",
    "from nltk.corpus.reader import ConllCorpusReader\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data\n",
    "test_df = pd.read_csv('data/sentiment-topic-test.tsv', delimiter='\\t')\n",
    "\n",
    "train_df = ConllCorpusReader('data/CONLL2003', 'train.txt', ['words', 'pos', 'ignore', 'chunk'])\n",
    "valid_df = ConllCorpusReader('data/CONLL2003', 'valid.txt', ['words', 'pos', 'ignore', 'chunk'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\kevin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Model Preparation\n",
    "nltk.download('vader_lexicon')\n",
    "vader_model = SentimentIntensityAnalyzer()\n",
    "nlp = spacy.load(\"en_core_web_sm\") # 'en_core_web_sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vader(textual_unit, \n",
    "              lemmatize=False, \n",
    "              parts_of_speech_to_consider=None,):\n",
    "    \"\"\"\n",
    "    Run VADER on a sentence from spacy\n",
    "    \n",
    "    :param str textual unit: a textual unit, e.g., sentence, sentences (one string)\n",
    "    (by looping over doc.sents)\n",
    "    :param bool lemmatize: If True, provide lemmas to VADER instead of words\n",
    "    :param set parts_of_speech_to_consider:\n",
    "    -None or empty set: all parts of speech are provided\n",
    "    -non-empty set: only these parts of speech are considered.\n",
    "    :param int verbose: if set to 1, information is printed\n",
    "    about input and output\n",
    "    \n",
    "    :rtype: dict\n",
    "    :return: vader output dict\n",
    "    \"\"\"\n",
    "    doc = nlp(textual_unit)\n",
    "    input_to_vader = []\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        for token in sentence:\n",
    "            to_add = token.text\n",
    "\n",
    "            if lemmatize:\n",
    "                to_add = token.lemma_ if token.lemma_ != '-PRON-' else token.text\n",
    "\n",
    "            if not parts_of_speech_to_consider:\n",
    "                input_to_vader.append(to_add)\n",
    "                continue\n",
    "            if token.pos_ in parts_of_speech_to_consider:\n",
    "                    input_to_vader.append(to_add)\n",
    "\n",
    "    return vader_model.polarity_scores(' '.join(input_to_vader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Begin VADER\n",
    "def vader_output_to_label(vader_output):\n",
    "    \"\"\"\n",
    "    map vader output e.g.,\n",
    "    {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.4215}\n",
    "    to one of the following values:\n",
    "    a) positive float -> 'positive'\n",
    "    b) 0.0 -> 'neutral'\n",
    "    c) negative float -> 'negative'\n",
    "    \n",
    "    :param dict vader_output: output dict from vader\n",
    "    \n",
    "    :rtype: str\n",
    "    :return: 'negative' | 'neutral' | 'positive'\n",
    "    \"\"\"\n",
    "    compound = vader_output['compound']\n",
    "    \n",
    "    if compound < 0:\n",
    "        return 'negative'\n",
    "    elif compound == 0.0:\n",
    "        return 'neutral'\n",
    "    elif compound > 0.0:\n",
    "        return 'positive'\n",
    "    \n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.0}) == 'neutral'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.01}) == 'positive'\n",
    "assert vader_output_to_label( {'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': -0.01}) == 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run VADER on the test data\n",
    "def test_vader(test_set, \n",
    "               lemmatize=False, \n",
    "               parts_of_speech_to_consider=None):\n",
    "    \"\"\"\n",
    "    Run VADER on a test set\n",
    "    \"\"\" \n",
    "    texts = []\n",
    "    vader_predictions = []\n",
    "    gold_labels = []\n",
    "\n",
    "    for index, row in test_set.iterrows():\n",
    "        text = row['text']\n",
    "        label = row['sentiment']\n",
    "        vader_output = run_vader(text, lemmatize=lemmatize, parts_of_speech_to_consider=parts_of_speech_to_consider)\n",
    "        vader_predictions.append(vader_output_to_label(vader_output))\n",
    "        gold_labels.append(label)\n",
    "        texts.append(text)\n",
    "\n",
    "    #evaluation\n",
    "    \n",
    "    print(classification_report(gold_labels, vader_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.25      0.33         4\n",
      "     neutral       0.40      0.67      0.50         3\n",
      "    positive       0.33      0.33      0.33         3\n",
      "\n",
      "    accuracy                           0.40        10\n",
      "   macro avg       0.41      0.42      0.39        10\n",
      "weighted avg       0.42      0.40      0.38        10\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_vader(test_df, lemmatize=True, parts_of_speech_to_consider={'VERB', 'ADJ', 'ADV'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn imports\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load training data\n",
    "train_data = pd.read_csv('data/Kaggle/sentiment-emotion-labelled_Dell_tweets.csv', delimiter=',')\n",
    "train_data = train_data[['Text', 'sentiment']]\n",
    "\n",
    "#replace Text by text\n",
    "train_data.columns = ['text', 'sentiment']\n",
    "\n",
    "#remove nan values\n",
    "train_data = train_data.dropna()\n",
    "\n",
    "#reshape test set\n",
    "test_df.head()\n",
    "test_df = test_df[['text', 'sentiment']]\n",
    "\n",
    "#add test set at the end of training set\n",
    "all_data = pd.concat([train_data, test_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {'text': [], 'sentiment': []}\n",
    "\n",
    "#append the text to the corresponding sentiment\n",
    "for index, row in all_data.iterrows():\n",
    "    train_dict['text'].append(str(nlp(row['text'])))\n",
    "    train_dict['sentiment'].append(row['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.57      1.00      0.73         4\n",
      "     neutral       0.00      0.00      0.00         3\n",
      "    positive       1.00      1.00      1.00         3\n",
      "\n",
      "    accuracy                           0.70        10\n",
      "   macro avg       0.52      0.67      0.58        10\n",
      "weighted avg       0.53      0.70      0.59        10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "count_vect = CountVectorizer(min_df=1, # If a token appears fewer times than this, across all documents, it will be ignored\n",
    "                            tokenizer=nltk.word_tokenize, # we use the nltk tokenizer\n",
    "                            stop_words=stopwords.words('english')) # stopwords are removed\n",
    "\n",
    "X_counts = count_vect.fit_transform(train_dict['text'])\n",
    "\n",
    "train_tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = train_tfidf_transformer.fit_transform(X_counts)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_tfidf, train_dict['sentiment'], test_size=10, shuffle=False)\n",
    "\n",
    "clf = MultinomialNB().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
